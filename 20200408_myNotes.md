# Udacity purpose
* créer vos propres opérateurs personnalisés pour effectuer des tâches telles que 
    * la mise en scène des données, 
    * le remplissage de l'entrepôt de données et 
    * la vérification des données comme étape finale.

* sont fourni un modèle de projet qui prend en charge toutes les importations
    * fournit quatre opérateurs vides qui doivent être implémentés dans les éléments fonctionnels d'un pipeline de données.
    * contient un ensemble de tâches qui doivent être liées pour obtenir un flux de données cohérent et sensé dans le pipeline.

Vous disposerez d'une classe d'assistants qui contient toutes les transformations SQL. Ainsi, vous n'aurez pas besoin d'écrire l'ETL vous-même, mais vous devrez l'exécuter avec vos opérateurs personnalisés.

```

    Conn Id: Enter aws_credentials.
    Conn Type: Enter Amazon Web Services.
    Login: Enter your Access key ID from the IAM User credentials you downloaded earlier.
    Password: Enter your Secret access key from the IAM User credentials you downloaded earlier.




    Conn Id: Enter redshift.
    Conn Type: Enter Postgres.
    Host: Enter the endpoint of your Redshift cluster, excluding the port at the end. 
    Schema: Enter dev. This is the Redshift database you want to connect to.
    Login: Enter awsuser.
    Password: Enter the password you created when launching your Redshift cluster.
    Port: Enter 5439.

```
## WARNING: Remember to DELETE your cluster each time you are finished working to avoid large, unexpected costs.
---
* DATASET:

    * Log data: s3://udacity-dend/log_data
    * Song data: s3://udacity-dend/song_data

* PROJECT TEMPLATE:  
    * **dag template** has all the imports and task templates in place, but the task dependencies have not been set
        
        * start_operator 
        * stage_events_to_redshift
        * stage_songs_to_redshift
        * load_songplays_table
        * load_user_dimension_table
        * load_song_dimension_table
        * load_artist_dimension_table
        * load_time_dimension_table
        * run_quality_checks
        * end_operator

    * **operators** folder with operator templates
         * from airflow.operators import
            * **StageToRedshiftOperator**, 
                * L'opérateur STAGE devrait pouvoir charger n'importe quel fichier au format JSON de S3 vers Amazon Redshift. L'opérateur crée et exécute une instruction SQL COPY sur la base des paramètres fournis. Les paramètres de l'opérateur doivent préciser où le fichier est chargé en S3 et quelle est la table cible.
                * Les paramètres doivent être utilisés pour distinguer les fichiers JSON. Une autre exigence importante de l'opérateur STAGE est de contenir un champ de modèle qui lui permet de charger des fichiers horodatés à partir de S3 en fonction du temps d'exécution et d'exécuter des remplissages.

            * **LoadFactOperator**,
                * Avec les opérateurs DIMENSION et FACT, vous pouvez utiliser la classe d'aide SQL fournie pour exécuter des transformations de données. La plupart de la logique se trouve dans les transformations SQL et l'opérateur est censé prendre en entrée une instruction SQL et une base de données cible sur lesquelles exécuter la requête. Vous pouvez également définir une table cible qui contiendra les résultats de la transformation.

            * **LoadDimensionOperator**,
                * Avec les opérateurs DIMENSION et FACT, vous pouvez utiliser la classe d'aide SQL fournie pour exécuter des transformations de données. La plupart de la logique se trouve dans les transformations SQL et l'opérateur est censé prendre en entrée une instruction SQL et une base de données cible sur lesquelles exécuter la requête. Vous pouvez également définir une table cible qui contiendra les résultats de la transformation.

                * Les chargements de DIMENSION sont souvent effectués avec le modèle de troncature-insertion où la table cible est vidée avant le chargement. Ainsi, vous pouvez aussi avoir un paramètre qui permet de passer d'un mode d'insertion à l'autre lors du chargement des dimensions. Les tables de faits sont généralement si massives qu'elles ne devraient permettre que la fonctionnalité de type append.

            * **DataQualityOperator**
                * L'opérateur final à créer est l'OPÉRATEUR DE LA QUALITÉ DES DONNÉES, qui est utilisé pour effectuer des contrôles sur les données elles-mêmes. La principale fonctionnalité de l'opérateur consiste à recevoir un ou plusieurs cas de test basés sur SQL ainsi que les résultats attendus et à exécuter les tests. Pour chaque test, le résultat du test et le résultat attendu doivent être vérifiés et s'il n'y a pas de correspondance, l'opérateur doit soulever une exception et la tâche doit être réessayée et échouer éventuellement.

                * Par exemple, un test pourrait être une instruction SQL qui vérifie si une certaine colonne contient des valeurs NULL en comptant toutes les lignes qui ont NULL dans la colonne. Nous ne voulons pas avoir de NULL, donc le résultat attendu serait 0 et le test comparerait le résultat de l'instruction SQL au résultat attendu.

    * **helper** class for the SQL transformations
        * sqlQueries

* utilize Airflow's built-in functionalities as **connections** and **hooks**
* All of the operators and task instances will run SQL statements against the Redshift database. However, **using parameters** wisely will allow you to build flexible, reusable, and configurable operators 
* 


```


.  
├── docker-compose.yml  
├── examples  
│   └── intro-example  
│       └── dags  
│           ├── config  
│           │   └── example_variables.json  
│           ├── example_twitter_dag.py  
│           ├── example_variables.py  
│           ├── __pycache__  
│           │   ├── example_twitter_dag.cpython-36.pyc  
│           │   ├── example_twitter_dag.cpython-37.pyc  
│           │   ├── example_variables.cpython-36.pyc  
│           │   ├── example_variables.cpython-37.pyc  
│           │   ├── tutorial.cpython-36.pyc  
│           │   └── tutorial.cpython-37.pyc  
│           └── tutorial.py  
├── LICENSE  
├── notebooks  
│   ├── docker-compose.yml  
│   └── Dockerfile  
├── README.md  
├── run_gcloud_example.sh  
└── stop_gcloud_example.sh  
```
![DAG](image/example-dag.png)

# AIRFLOW

https://www.applydatascience.com/airflow/set-up-airflow-env-with-docker/

https://pypi.org/project/apache-airflow/


todo:
* Update Images version airflow postgres  and other in dockerfile
    * go to docker HUb and look for your feature and tags and last release
* _ras: add packages if needed_
* _ras:create a file setup environement and change in the docker-compose.yml_
    POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB
* step2 : regarder pour plus de configuration
* step4 : rechercher jigga template pour airflow et bash
* _ras: script to run and to stop airflow UI_
* _ras: setup connection in a script?_
* setup variables environement aws, avec des variables d'environnemt

* travailler en local
    * importer les fichiers dans le docker pour ne pas se connnecter a s3 et redshift
    * ecrire les default_args: parametrer adresse mails

* tests si erreurs, si envoie d'un mail

* architecture du fichier sparkify.py
    * import
    * default_args
    * dag = DAG()
    * worflow des Tasks
    * dependencies





## Typical worflow

* Download_data > send_data_to_processing > momitor_processinf > generate_report > send_email

* collect data right production ETL data pipeline so have a clean quality data 
* Move/Store 
* Explore/Transform

* Manage and maintain Airflow
    * Installation extra feature `pip install apache-airflow[postgres,s3]`
    * Install Extra Packages: pick and choose to integrate with different cloud environment and different databases
* Docker allows to manage environments

## Write an Airflow DAG

### Step1: importing modules
* Import Python dependencies needed for the worflow
* Import **customize library** too
       ```python 
        from datetime import timedelta

        import airflow
        from airflow import DAG
        from airflow.operators.bash_operator import BashOperator
        ```
### Step2: Default Arguments
* Define default and DAG-specific arguments
* Python disctionnary, apply all the tasks in the airflow
* Airflow take care of failure not as cron
* check to airflow website for more details

        ```python
        default_args = {
            ## name of the owner DAG
            'owner': 'airflow',     
            ## start of the task, determine the first task instant, # must be a specific day in the past, Airflow create a list of tasks until the end
            'start_date': airflow.utils.dates.days_ago(2), 
            # 'end_date': datetime(2018, 12, 30),
            ## if we wanr rerun from tha last tine, setup = TRUE, run all each time = FALSE
            'depends_on_past': False,
            # email setup
            'email': ['youremail@gmail.com'],
            'email_on_failure': True,
            'email_on_retry': True,
            # If a task fails, retry it once after waiting
            # at least 5 minutes
            'retries': 1,
            'retry_delay': timedelta(minutes=5),
            }
        ```
### Step3: Instantiate a DAG
* Give the DAG name, configure the schedule, set the DAG settings

        ```python
        dag = DAG(
            'tutorial',
            default_args=default_args,
            description='A simple tutorial DAG',
            ## how often the dag is trigger and executed(@once, @hourly, ... , None)
            # Continue to run DAG once per day 
            schedule_interval=timedelta(days=1),
               )

        ```
### Step4: Tasks
* To lay out all the tasks in the worflow
* on parle de jigga template? server send file to client

        ```python
        # t1, t2 and t3 are examples of tasks created by instantiating operators
        # BashOperator to run Bash command
        t1 = BashOperator(          # print the current date
            task_id='print_date',
            bash_command='date',
            dag=dag,
        )

        t2 = BashOperator(          # sleep for five second
            task_id='sleep',
            depends_on_past=False,
            bash_command='sleep 5',
            dag=dag,
        )
        # useful for generate data ~~jigga template?
        ## airflow variable (ds, macros)
        ## custom variables (my_param), we pass value in. 
        templated_command = """
        {% for i in range(5) %}
            echo "{{ ds }}"
            echo "{{ macros.ds_add(ds, 7)}}"
            echo "{{ params.my_param }}"
        {% endfor %}
        """

        t3 = BashOperator(          # run the bash command "templated_command"
            task_id='templated',
            depends_on_past=False,
            bash_command=templated_command,
            params={'my_param': 'Parameter I passed in'}, # to be use in templated_command
            dag=dag,
        )
        ```
### step5: Setting up Depemdencies
* Set the dependencies or the order in which the tasks should be executed.
    * t1.set_downstream(t2)
    * t1 >> t2
    * t1 >> [t2, t3]

## Airflow concept
* nodes == tasks
* edges == dependencies.
* a travers un node, data est transforempuis transporte a travers edge dans une seule direction

### operators ans Tasks
* DAGs do not perform any actual computation. Instead, Operators determine what actually gets done.
* Task: Once an operator is instantiated, it is referred to as a “task”. An operator describes a single task in a workflow.
* Instantiating a task requires providing a unique task_id and DAG container
    A DAG is a container that is used to organize tasks and set their execution context.
* Operators are classified into three categories:
    * **Sensors**: a certain type of operator that will keep **running until a certain criteria is met**. Example include waiting for a certain time, external file, or upstream data source. Often the first, pre-check condition before run any computation
        * HdfsSensor: Waits for a file or folder to land in HDFS
        * NamedHivePartitionSensor: check whether the most recent partition of a Hive table is available for downstream processing.
    * **Operators**: triggers a certain action (e.g. run a bash command, execute a python function, or execute a Hive query, etc). In ETL, operators are the transform step. 
        * BashOperator: executes a bash command
        * PythonOperator: calls an arbitrary Python function
        * HiveOperator: executes hql code or hive script in a specific Hive database.
        * BigQueryOperator: executes Google BigQuery SQL queries in a specific BigQuery database
    * **Transfers**: moves data from one location to another.
        * MySqlToHiveTransfer: Moves data from MySql to Hive.
        * S3ToRedshiftTransfer: load files from s3 to Redshift
### Operators
* There are more operators being added by the community. airflow/contrib/ directory to look for the community added operators.
* All operators are derived from `BaseOperator` and acquire much functionality through inheritance. Contributors can extend `BaseOperator` class to create custom operators as they see fit

            ```python
            class HiveOperator(BaseOperator):
            """
            HiveOperator inherits from BaseOperator
            """
            ```
### dependencies
* After defining a DAG, and instantiate all the tasks, you can then set the dependencies or the order in which the tasks should be executed.
    * the set_upstream and set_downstream operators.
    * the bitshift operators << and >>

### DagRuns and TaskInstances
* A key concept in Airflow is the *execution_time*. The execution times begin at the DAG’s start_date and repeat every schedule_interval.
* For each execution_time, a DagRun is created and operates under the context of that execution time. A DagRun is simply a DAG that has a specific execution time.
    * **DagRuns** are DAGs that runs at a certain time.
    * **TaskInstances** are the task belongs to that DagRuns. 
* Each DagRun and TaskInstance is associated with an entry in Airflow’s metadata database that logs their state

### Variables

* Variables are key-value stores in Airflow’s metadata database.
* It is used to store and retrieve arbitrary content or settings from the metadata database.
* Variables are mostly used to store static values like:
       * config variables
       * a configuration file
       * list of tables
       * list of IDs to dynamically generate tasks from
* Separate the constants and variables from pipeline code:
       * It is useful to have some variables or configuration items accessible and modifiable through the UI.

* Working with Variables
    * Variables can be listed, created, updated and deleted from the UI (Admin -> Variables).
    * In addition, json settings files can be bulk uploaded through the UI. 
    ```json
    {
        "example_variables_config": {
            "var1": "value1",
            "var2": [1, 2, 3],
            "var3": {
                    "k": "value3"
            }
        }
    }
    ```
* Restrict the number of Airflow variables in your DAG
    * Since Airflow Variables are stored in Metadata Database, so any call to variables would mean a connection to Metadata DB.
       * Instead of storing a large number of variable in your DAG, which may end up saturating the number of allowed connections to your database.
       * It is recommended you store all your DAG configuration inside a single Airflow variable with JSON value.
* how to do: admin > variables > import file > ./examples/intro-example/dags/config/example-variables.json > click on import
    * json file, mean dictionnary
    * name of the field: example_varaiables_config
    * "var1": "value1"
    * etc ... see up 

## Access variables through Airflow command line

### get value of var1
docker-compose run --rm webserver airflow variables --get var1

### set value of var4
docker-compose run --rm webserver airflow variables --set var4 value4]

### import variable json file
docker-compose run --rm webserver airflow variables --import /usr/local/airflow/dags/config/example_variables.json


## Acces connections trough Airflow command line


sudo docker-compose exec webserver airflow variables -i variables.json

sudo docker-compose exec webserver airflow connections -a --conn_id examplessh --conn_uri ssh://user:pass@serverip --conn_extra '{"key_file":"/usr/local/airflow/id_rsa", "no_host_key_check":true}'

sudo docker-compose exec webserver airflow connections -a --conn_id examples3 --conn_uri s3://my-test-bucket --conn_extra '{"aws_access_key_id":"AAAAAAA", "aws_secret_access_key":"bbbbbbb", "bucket_name":"mybucket"}'

