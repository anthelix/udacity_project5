[anthelix]% tree
.
├── docker-compose.yml
├── examples
│   └── intro-example
│       └── dags
│           ├── config
│           │   └── example_variables.json
│           ├── example_twitter_dag.py
│           ├── example_variables.py
│           ├── __pycache__
│           │   ├── example_twitter_dag.cpython-36.pyc
│           │   ├── example_twitter_dag.cpython-37.pyc
│           │   ├── example_variables.cpython-36.pyc
│           │   ├── example_variables.cpython-37.pyc
│           │   ├── tutorial.cpython-36.pyc
│           │   └── tutorial.cpython-37.pyc
│           └── tutorial.py
├── LICENSE
├── notebooks
│   ├── docker-compose.yml
│   └── Dockerfile
├── README.md
├── run_gcloud_example.sh
└── stop_gcloud_example.sh



# AIRFLOW


https://www.applydatascience.com/airflow/set-up-airflow-env-with-docker/

https://pypi.org/project/apache-airflow/


todo:
* Update Images version airflow postgres  and other in dockerfile
    * go to docker HUb and look for your feature and tags and last release
* add packages if needed
    Redshift? 
* create a file setup environement and change in the docker-compose.yml
    POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB
* step2 : regarder pour plus de configuration
* step4 : rechercher jigga template pour airflow et bash
* script to run and to stop airflow UI
* setup connection in a script?
* 

## Typical worflow

* Download_data > send_data_to_processing > momitor_processinf > generate_report > send_email

* collect data right production ETL data pipeline so have a clean quality data 
* Move/Store 
* Explore/Transform

* Manage and maintain Airflow
    * Installation extra feature `pip install apache-airflow[postgres,s3]`
    * Install Extra Packages: pick and choose to integrate with different cloud environment and different databases
* Docker allows to manage environments

## Write an Airflow DAG

### Step1: importing modules
* Import Python dependencies needed for the worflow
* Import **customize library** too
       ```python 
        from datetime import timedelta

        import airflow
        from airflow import DAG
        from airflow.operators.bash_operator import BashOperator
        ```
### Step2: Default Arguments
* Define default and DAG-specific arguments
* Python disctionnary, apply all the tasks in the airflow
* Airflow take care of failure not as cron
* check to airflow website for more details

        ```python
        default_args = {
            ## name of the owner DAG
            'owner': 'airflow',     
            ## start of the task, determine the first task instant, # must be a specific day in the past, Airflow create a list of tasks until the end
            'start_date': airflow.utils.dates.days_ago(2), 
            # 'end_date': datetime(2018, 12, 30),
            ## if we wanr rerun from tha last tine, setup = TRUE, run all each time = FALSE
            'depends_on_past': False,
            # email setup
            'email': ['chatagns@gmail.com'],
            'email_on_failure': True,
            'email_on_retry': True,
            # If a task fails, retry it once after waiting
            # at least 5 minutes
            'retries': 1,
            'retry_delay': timedelta(minutes=5),
            }
        ```
### Step3: Instantiate a DAG
* Give the DAG name, configure the schedule, set the DAG settings

        ```python
        dag = DAG(
            'tutorial',
            default_args=default_args,
            description='A simple tutorial DAG',
            ## how often the dag is trigger and executed(@once, @hourly, ... , None)
            # Continue to run DAG once per day 
            schedule_interval=timedelta(days=1),
               )

        ```
### Step4: Tasks
* To lay out all the tasks in the worflow
* on parle de jigga template? server send file to client

        ```python
        # t1, t2 and t3 are examples of tasks created by instantiating operators
        # BashOperator to run Bash command
        t1 = BashOperator(          # print the current date
            task_id='print_date',
            bash_command='date',
            dag=dag,
        )

        t2 = BashOperator(          # sleep for five second
            task_id='sleep',
            depends_on_past=False,
            bash_command='sleep 5',
            dag=dag,
        )
        # useful for generate data ~~jigga template?
        ## airflow variable (ds, macros)
        ## custom variables (my_param), we pass value in. 
        templated_command = """
        {% for i in range(5) %}
            echo "{{ ds }}"
            echo "{{ macros.ds_add(ds, 7)}}"
            echo "{{ params.my_param }}"
        {% endfor %}
        """

        t3 = BashOperator(          # run the bash command "templated_command"
            task_id='templated',
            depends_on_past=False,
            bash_command=templated_command,
            params={'my_param': 'Parameter I passed in'}, # to be use in templated_command
            dag=dag,
        )
        ```
### step5: Setting up Depemdencies
* Set the dependencies or the order in which the tasks should be executed.
    * t1.set_downstream(t2)
    * t1 >> t2
    * t1 >> [t2, t3]

## Airflow concept
* nodes == tasks
* edges == dependencies.
* a travers un node, data est transforempuis transporte a travers edge dans une seule direction

### operators ans Tasks
* DAGs do not perform any actual computation. Instead, Operators determine what actually gets done.
* Task: Once an operator is instantiated, it is referred to as a “task”. An operator describes a single task in a workflow.
* Instantiating a task requires providing a unique task_id and DAG container
    A DAG is a container that is used to organize tasks and set their execution context.
* Operators are classified into three categories:
    * **Sensors**: a certain type of operator that will keep **running until a certain criteria is met**. Example include waiting for a certain time, external file, or upstream data source. Often the first, pre-check condition before run any computation
        * HdfsSensor: Waits for a file or folder to land in HDFS
        * NamedHivePartitionSensor: check whether the most recent partition of a Hive table is available for downstream processing.
    * **Operators**: triggers a certain action (e.g. run a bash command, execute a python function, or execute a Hive query, etc). In ETL, operators are the transform step. 
        * BashOperator: executes a bash command
        * PythonOperator: calls an arbitrary Python function
        * HiveOperator: executes hql code or hive script in a specific Hive database.
        * BigQueryOperator: executes Google BigQuery SQL queries in a specific BigQuery database
    * **Transfers**: moves data from one location to another.
        * MySqlToHiveTransfer: Moves data from MySql to Hive.
        * S3ToRedshiftTransfer: load files from s3 to Redshift
### Operators
* There are more operators being added by the community. airflow/contrib/ directory to look for the community added operators.
* All operators are derived from `BaseOperator` and acquire much functionality through inheritance. Contributors can extend `BaseOperator` class to create custom operators as they see fit

            ```python
            class HiveOperator(BaseOperator):
            """
            HiveOperator inherits from BaseOperator
            """
            ```
### dependencies
* After defining a DAG, and instantiate all the tasks, you can then set the dependencies or the order in which the tasks should be executed.
    * the set_upstream and set_downstream operators.
    * the bitshift operators << and >>

### DagRuns and TaskInstances
* A key concept in Airflow is the *execution_time*. The execution times begin at the DAG’s start_date and repeat every schedule_interval.
* For each execution_time, a DagRun is created and operates under the context of that execution time. A DagRun is simply a DAG that has a specific execution time.
    * **DagRuns** are DAGs that runs at a certain time.
    * **TaskInstances** are the task belongs to that DagRuns. 
* Each DagRun and TaskInstance is associated with an entry in Airflow’s metadata database that logs their state

### Variables

* Variables are key-value stores in Airflow’s metadata database.
* It is used to store and retrieve arbitrary content or settings from the metadata database.
* Variables are mostly used to store static values like:
       * config variables
       * a configuration file
       * list of tables
       * list of IDs to dynamically generate tasks from
* Separate the constants and variables from pipeline code:
       * It is useful to have some variables or configuration items accessible and modifiable through the UI.

* Working with Variables
    * Variables can be listed, created, updated and deleted from the UI (Admin -> Variables).
    * In addition, json settings files can be bulk uploaded through the UI. 
    ```json
    {
        "example_variables_config": {
            "var1": "value1",
            "var2": [1, 2, 3],
            "var3": {
                    "k": "value3"
            }
        }
    }
    ```
* Restrict the number of Airflow variables in your DAG
    * Since Airflow Variables are stored in Metadata Database, so any call to variables would mean a connection to Metadata DB.
       * Instead of storing a large number of variable in your DAG, which may end up saturating the number of allowed connections to your database.
       * It is recommended you store all your DAG configuration inside a single Airflow variable with JSON value.
* how to do: admin > variables > import file > ./examples/intro-example/dags/config/example-variables.json > click on import
    * json file, mean dictionnary
    * name of the field: example_varaiables_config
    * "var1": "value1"
    * etc ... see up 

## Access variables through Airflow command line

### get value of var1
docker-compose run --rm webserver airflow variables --get var1

### set value of var4
docker-compose run --rm webserver airflow variables --set var4 value4]

### import variable json file
docker-compose run --rm webserver airflow variables --import /usr/local/airflow/dags/config/example_variables.json


## Acces connections trough Airflow command line


sudo docker-compose exec webserver airflow variables -i variables.json

sudo docker-compose exec webserver airflow connections -a --conn_id examplessh --conn_uri ssh://user:pass@serverip --conn_extra '{"key_file":"/usr/local/airflow/id_rsa", "no_host_key_check":true}'

sudo docker-compose exec webserver airflow connections -a --conn_id examples3 --conn_uri s3://my-test-bucket --conn_extra '{"aws_access_key_id":"AAAAAAA", "aws_secret_access_key":"bbbbbbb", "bucket_name":"mybucket"}'


